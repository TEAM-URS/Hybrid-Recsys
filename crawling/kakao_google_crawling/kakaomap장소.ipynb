{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_name = []\n",
    "p_address = []\n",
    "p_category = []\n",
    "p_rating = []\n",
    "p_review = []\n",
    "p_tag = []\n",
    "p_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global load_wb, review_num\n",
    "\n",
    "    # 검색할 목록\n",
    "    place_infos = ['울산 공원', '울산 볼링장', '울산 명소', '울산 만화카페', '울산 멀티방', '울산 방탈출', '울산 보드카페', '울산 코인노래방', '울산 숙소']\n",
    "\n",
    "    for i, place in enumerate(place_infos):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('lang=ko_KR')\n",
    "        chromedriver_path = \"chromedriver\"\n",
    "        driver = webdriver.Chrome(os.path.join(os.getcwd(), chromedriver_path), options=options)  # chromedriver 열기\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.get('https://map.kakao.com/') \n",
    "        search(driver, place)\n",
    "        driver.quit()\n",
    "    print(\"finish\")\n",
    "\n",
    "\n",
    "def search(driver, place):\n",
    "\n",
    "    search_area = driver.find_element(By.XPATH, '//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
    "    search_area.send_keys(place)  # 검색어 입력\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"info.main.options\"]/li[1]/a').send_keys(Keys.ENTER)\n",
    "\n",
    "    # 검색된 정보가 있는 경우에만 탐색\n",
    "    # 1번 페이지 place list 읽기\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
    "\n",
    "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
    "    crawling(driver, place, place_lists)\n",
    "    search_area.clear()\n",
    "\n",
    "    # 우선 더보기 클릭해서 2페이지\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)\n",
    "\n",
    "        # 2~ 5페이지 읽기\n",
    "        for i in range(2, 6):\n",
    "            # 페이지 넘기기\n",
    "            xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "            if i == 5:\n",
    "                xPath = '//*[@id=\"info.search.page.next\"]'\n",
    "            driver.find_element(By.XPATH, xPath).send_keys(Keys.ENTER)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "\n",
    "            crawling(driver, place, place_lists)\n",
    "        \n",
    "        for i in range(1,7):\n",
    "          if i == 6:\n",
    "            for i in range(3, 5):\n",
    "              # 페이지 넘기기\n",
    "              xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "              driver.find_element(By.XPATH, xPath).send_keys(Keys.ENTER)\n",
    "\n",
    "              html = driver.page_source\n",
    "              soup = BeautifulSoup(html, 'html.parser')\n",
    "              place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "\n",
    "              crawling(driver, place, place_lists)\n",
    "          else:\n",
    "            for i in range(3, 7):\n",
    "              # 페이지 넘기기\n",
    "              xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "              if i == 6:\n",
    "                  xPath = '//*[@id=\"info.search.page.next\"]'\n",
    "              driver.find_element(By.XPATH, xPath).send_keys(Keys.ENTER)\n",
    "\n",
    "              html = driver.page_source\n",
    "              soup = BeautifulSoup(html, 'html.parser')\n",
    "              place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "\n",
    "              crawling(driver, place, place_lists)\n",
    "\n",
    "    except:\n",
    "        return\n",
    "    finally:\n",
    "        search_area.clear()\n",
    "\n",
    "\n",
    "def crawling(driver, place_key, place_lists):\n",
    "    while_flag = False\n",
    "    for i, place in enumerate(place_lists):\n",
    "        if place.select('.head_item > .tit_name > .link_name')[0]['title'] == '공원아파트':\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
    "            p_name.append(place_name)\n",
    "            place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
    "            p_address.append(place_address)\n",
    "            place_rating = place.select('.rating > .score > em')[0].text\n",
    "            p_rating.append(place_rating)\n",
    "            try:\n",
    "                place_category = place.select('span.subcategory.clickable')[0].text\n",
    "            except:\n",
    "                place_category=''\n",
    "            p_category.append(place_category)\n",
    "            place_more = place.select('.moreview')[0]['href']\n",
    "            p_url.append(place_more)\n",
    "        \n",
    "\n",
    "            detail_page_xpath = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
    "            driver.find_element(By.XPATH, detail_page_xpath).send_keys(Keys.ENTER)\n",
    "            driver.switch_to.window(driver.window_handles[-1])  # 상세정보 탭으로 변환\n",
    "\n",
    "            reviews = extract_review(driver, place_key, place_name)\n",
    "            p_review.append(reviews)\n",
    "            tags = extract_tag(driver, place_name)\n",
    "            p_tag.append(tags)\n",
    "\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
    "\n",
    "\n",
    "def extract_review(driver, place_key, place_id):\n",
    "    review = []\n",
    "    try:\n",
    "        if driver.find_element(By.CSS_SELECTOR, '#mArticle > div.cont_essential > div:nth-child(1) > div.place_details > div > div > a:nth-child(3) > span.color_g').text == '(0)':\n",
    "            return '리뷰 없음'\n",
    "        else:\n",
    "            return f'./kakao_reviews/{place_key}/{place_id}.csv'\n",
    "    except:\n",
    "            print('리뷰 뽑는 중 오류')\n",
    "            return '리뷰 없음'\n",
    "    # review_df=pd.DataFrame(columns={'p_id','u_id','u_rate','review'})\n",
    "    \n",
    "    # while True:\n",
    "    #     try:\n",
    "    #         if driver.find_element(By.CSS_SELECTOR, '#mArticle > div.cont_evaluation > div.evaluation_review > a').text == '후기 더보기':\n",
    "    #             driver.find_element(By.CSS_SELECTOR, '#mArticle > div.cont_evaluation > div.evaluation_review > a').click()\n",
    "    #         else:\n",
    "    #             break\n",
    "    #     except:\n",
    "    #         break\n",
    "    #     sleep(0.5)\n",
    "    \n",
    "    # html = driver.page_source\n",
    "    # soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # try:\n",
    "    #     review_lists = soup.find('ul', {'class':'list_evaluation'}).find_all('li')\n",
    "    # except:\n",
    "    #     return ''\n",
    "    \n",
    "    # p_name = []\n",
    "    # u_name = []\n",
    "    # u_rating = []\n",
    "    # u_comment = []\n",
    "    \n",
    "    # for review_list in review_lists:\n",
    "    #     try:\n",
    "    #         user_id = review_list.find('a', {'class':'link_user'}).text\n",
    "    #     except:\n",
    "    #         user_id = ''\n",
    "    #         continue\n",
    "        \n",
    "    #     try:\n",
    "    #         value = review_list.find('div', {'class':'star_info'}).find('div').find('span').find('span')['style']\n",
    "    #         value = value.replace('width:', '')\n",
    "    #         value = value.replace('%;', '')\n",
    "    #         user_rating = int(value)/20\n",
    "    #     except:\n",
    "    #         user_rating = ''\n",
    "    #         continue\n",
    "        \n",
    "    #     try:\n",
    "    #         user_comment = review_list.find('p', {'class':'txt_comment'}).find('span').text\n",
    "    #     except:\n",
    "    #         user_comment = ''\n",
    "    #         continue\n",
    "    #     p_name.append(place_id)\n",
    "    #     u_name.append(user_id)\n",
    "    #     u_rating.append(user_rating)\n",
    "    #     u_comment.append(user_comment)\n",
    "        \n",
    "    #     print('place_id:', place_id)\n",
    "    #     print('user_id:', user_id)\n",
    "    #     print('score:', user_rating)\n",
    "    #     print('comment:', user_comment)\n",
    "    \n",
    "    # review_df = pd.DataFrame({\n",
    "    #     'place_id':p_name,\n",
    "    #     'user_id':u_name,\n",
    "    #     'score':u_rating,\n",
    "    #     'comment':u_comment\n",
    "    # },)\n",
    "\n",
    "    # review_df.to_csv(f'./kakao_reviews/{place_key}/{place_id}.csv', index = False, header=True, encoding='utf-8-sig')\n",
    "    # return f'./kakao_reviews/{place_key}/{place_id}.csv'\n",
    "\n",
    "def extract_tag(driver, place_name):\n",
    "    tags = []\n",
    "    try:\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # 첫 페이지 리뷰 목록 찾기\n",
    "        tag_lists = soup.select('.tag_g > a')\n",
    "\n",
    "        tags = []\n",
    "        # 리뷰가 있는 경우\n",
    "        if len(tag_lists) != 0:\n",
    "            for i, tag in enumerate(tag_lists):\n",
    "                if len(tag) != 0:\n",
    "                    tags.append(tag.text)\n",
    "\n",
    "            return tags\n",
    "        else:\n",
    "            return '태그 없음'\n",
    "    except:\n",
    "        print('태그 뽑는 중 오류')\n",
    "        return '태그 없음'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82107\\AppData\\Local\\Temp\\ipykernel_31620\\667134887.py:11: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(os.path.join(os.getcwd(), chromedriver_path), options=options)  # chromedriver 열기\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 뽑는 중 오류\n",
      "리뷰 뽑는 중 오류\n",
      "리뷰 뽑는 중 오류\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260 1260 1260 1260 1260 1260\n"
     ]
    }
   ],
   "source": [
    "print(len(p_name), len(p_address), len(p_rating), len(p_tag), len(p_review), len(p_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1259\n",
    "p_review.insert(index, '리뷰 없음')\n",
    "p_tag.insert(index, '태그 없음')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해변의터마루 ./kakao_reviews/울산 숙소/해변의터마루.csv\n"
     ]
    }
   ],
   "source": [
    "index = 1323\n",
    "print(p_name[index], p_review[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1259\n"
     ]
    }
   ],
   "source": [
    "for i in range(1326):\n",
    "  try:\n",
    "    if p_review[i] == '리뷰 없음':\n",
    "      pass\n",
    "    else:\n",
    "      value = re.search('kakao_reviews/.+/(.+)\\.csv', p_review[i]).group(1)\n",
    "      if p_name[i] == value:\n",
    "        continue\n",
    "      else:\n",
    "        print(i)\n",
    "        break\n",
    "  except:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "  'p_name':p_name,\n",
    "  'p_address':p_address,\n",
    "  'p_rating':p_rating,\n",
    "  'p_category':p_category,\n",
    "  'p_tag':p_tag,\n",
    "  'p_review':p_review,\n",
    "  'p_url':p_url\n",
    "},)\n",
    "df.to_csv('kakaoMap.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "041d1bf15cd7204c11d27e64d7e3f888abac55cd4faa8256afeeb4489e7f0daf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
